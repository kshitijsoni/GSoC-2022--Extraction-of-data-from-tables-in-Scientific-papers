# -*- coding: utf-8 -*-
"""image_classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14-LgBRQZyC7A2lIlYhY9udrao-dVRP6S

**Basic Requirements**
"""

import sys
import os
import tensorflow as tf
import keras
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense, Activation
from keras.layers import  Convolution2D, MaxPooling2D
from keras import backend as K
import datetime
import time
import matplotlib.pyplot as plt
from keras.layers import Conv2D, MaxPooling2D
from keras import regularizers
import random
from keras.layers import Conv2D, MaxPooling2D
from keras import regularizers
from keras.callbacks import TensorBoard
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from keras.callbacks import TensorBoard
import numpy as np
import itertools
import pickle
from sklearn.metrics import confusion_matrix

"""**load Datasets**"""

data_train = #input path
data_validate = #input path
data_test = #input path

"""
Parameters
"""
#epochs=60
length, height = 224, 224
batch_size = 32
nb_train_samples = 7039
nb_validation_samples = 1508
nb_test_samples = 1508
filterConv1 = 32
filterConv2 = 64
filter_size1 = (3, 3)
filter_size2 = (2, 2)
size_pool = (2, 2)
classes = 2

#MODEL NM, model MM with changes of hashtag, MM2 introduces the early stopping
seed = 1
random.seed(seed)
os.environ['PYTHONHASHSEED']='seed'
tf.set_random_seed(seed)
np.random.seed(seed)

#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)
#sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options))

sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))

#Image preparation

training_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)


training_generator = training_datagen.flow_from_directory(
    data_train,
    target_size=(height, length),
    batch_size=batch_size,
    class_mode='binary',
    seed = seed)

validation_generator = test_datagen.flow_from_directory(
    data_validate,
    target_size=(height, length),
    batch_size=batch_size,
    class_mode='binary',
    seed = seed)

#test_batches = ImageDataGenerator().flow_from_directory(
#    data_test,
#    target_size=(heigth, length),
#    classes=['linear','nonlinear'],
#    seed = seed,
#    batch_size=batch_size)

act = function(init='zero', weights=None)

epochs=20
dense_layers = [1,2]
layer_sizes = [64,256]
conv_layers = [1,2]
lr = [1e-3, 1e-4, 1e-5, 1e-6]
drop_rate =[0.4, 0.5, 0.6, 0.7]

for dense_layer in dense_layers:
    for layer_size in layer_sizes:
        for conv_layer in conv_layers:
            for learning in lr:
                for dropout_rate in drop_rate:

                    model = None
                    del model
                    K.clear_session()
                    K.set_session(sess)
                    optimizer=optimizers.Adam(lr=learning)

                    filenames= os.listdir (#path) # get all files' and folders' names in the current directory

                    result = []
                    for filename in filenames:
                        result.append(filename)

                    NAME = "UU-{}-conv-{}-nodes-{}-dense-{}-learning-{}-drop".format(conv_layer, layer_size, dense_layer, learning, dropout_rate)
                    NAME_MC = "UU-{}-conv-{}-nodes-{}-dense-{}-learning-{}-drop-best.h5".format(conv_layer, layer_size, dense_layer, learning, dropout_rate)
                    NAME_MODEL = "UU-{}-conv-{}-nodes-{}-dense-{}-learning-{}-drop-model.h5".format(conv_layer, layer_size, dense_layer, learning, dropout_rate)


                    if NAME in result:
                        pass
                    else:

                        model = Sequential()


                        model.add(Conv2D(filterConv1, filter_size1, padding ="same", kernel_regularizer=regularizers.l1_l2(0.01), 
                           activity_regularizer=regularizers.l2(0.01), input_shape=(length, heigth, 3), activation='relu'))
                        model.add(MaxPooling2D(pool_size = size_pool))

                        model.add(Conv2D(filterConv1, filter_size1, padding ="same", input_shape=(length, heigth, 3), activation='relu'))
                        model.add(MaxPooling2D(pool_size= size_pool))

                        for l in range(conv_layer-1):
                            model.add(Conv2D(filterConv2, filter_size2, kernel_regularizer=regularizers.l1_l2(0.01), 
                                            activity_regularizer=regularizers.l2(0.01), padding ="same"))
                            model.add(MaxPooling2D(pool_size= size_pool))

                        model.add(Dropout(dropout_rate))

                        for l in range(conv_layer-1):


                            model.add(Conv2D(filterConv2, filter_size2, padding ="same"))
                            model.add(MaxPooling2D(pool_size= size_pool))
                            model.add(Dropout(dropout_rate))

                        model.add(Flatten())

                        for _ in range(dense_layer):
                            model.add(Dense(layer_size))
                            model.add(Activation('relu'))


                        model.add(Dense(1))
                        model.add(Activation('sigmoid'))


                        model.compile(loss='binary_crossentropy',
                                      optimizer=optimizers.Adam(lr=learning),
                                      metrics=['accuracy'],
                                      )


                        tensorboard = TensorBoard(log_dir= #input.format(NAME))
                        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=1e-4)  
                        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)
                        mc = ModelCheckpoint(filepath='{}'.format(NAME_MC), monitor='val_loss', verbose=0, save_best_only=True,
                                            save_weights_only=False,
                                            mode='min',
                                            period=5)
                        callbacks_list = [reduce_lr, tensorboard,early_stop,mc]

                        print(NAME)
                        history = model.fit_generator(
                                                        training_generator,
                                                        steps_per_epoch=nb_train_samples // batch_size,
                                                        validation_steps=nb_validation_samples // batch_size,
                                                        epochs=epochs,
                                                        validation_data=validation_generator,
                                                        callbacks=callbacks_list)

                        ## summarize history for accuracy
                        plt.plot(history.history['acc'])
                        plt.plot(history.history['val_acc'])
                        plt.title('model accuracy')
                        plt.ylabel('accuracy')
                        plt.xlabel('epoch')
                        plt.legend(['train', 'test'], loc='upper left')
                        plt.show()

                        ## summarize history for loss
                        plt.plot(history.history['loss'])
                        plt.plot(history.history['val_loss'])
                        plt.title('model loss')
                        plt.ylabel('loss')
                        plt.xlabel('epoch')
                        plt.legend(['train', 'test'], loc='upper left')
                        plt.show()

                        #import pickle
                        #pickle_out = open('history_pickle_{}'.format(NAME),"wb")
                        #pickle.dump(history.history, pickle_out)
                        #pickle_out.close()

                        model.save('{}'.format(NAME_MODEL))